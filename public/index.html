<!DOCTYPE html>
<html lang="es">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ML-Explainer: Una Aventura Interactiva</title>
  <link rel="stylesheet" href="css/styles.css">
</head>

<body>

  <header>
    <h1>ML-Explainer: Una Aventura Interactiva</h1>
    <nav>
      <a href="#decision-trees">Árboles de Decisión</a>
      <a href="#reinforcement-learning">Reinforcement Learning</a>
      <a href="#conclusion">Conclusión</a>
    </nav>
  </header>

  <main>
    <section id="portada">
      <h3 class="bienvenida">¡Bienvenido a ML-Explainer!</h3>
      <h2 class="subtitulo-portada">Comprende el Machine Learning de Forma Interactiva</h2>
      <p>Esta página es un laboratorio interactivo diseñado para desmitificar dos de los conceptos más poderosos del
        Machine Learning. Aquí, no solo leerás sobre los algoritmos detrás de este, sino que podrás jugar con
        ellos, ajustar sus parámetros y ver cómo «piensan» en tiempo real.</p>
      <div class="autores">
        <span>Zihao Xiao</span>
        <span>Jesús Tapia</span>
        <span>Benjamín Fernández</span>
      </div>
    </section>

    <section id="decision-trees">
      <h2>🌳 Árboles de Decisión</h2>
      <p>Dentro del Machine Learning, los árboles de decisión son de los algoritmos de <strong>«Aprendizaje
          Supervisado»</strong> más intuitivos
        que hay. Estos son utilizados principalmente para tareas de <strong>«Clasificación»</strong>, como predecir una
        categoría, y tareas de <strong>«Regresión»</strong>, como predecir un valor numérico, imitando la forma en que
        los humanos toman
        decisiones mediante una serie de preguntas.</p>
      <h3>La Idea en Simple: Un Juego de Preguntas Inteligente</h3>
      <div class="explicacion-arbol">
        <p>
          Imagina que quieres decidir si jugar o no jugar tenis, para esto un árbol de decisión te guiará por medio de
          preguntas de manera aleatoria.
          una serie de preguntas.
          Bajo este contexto, se debe considerar que el algoritmo buscará la «pregunta más inteligente» en cada paso, es
          decir, aquella que mejor
          divida los datos en grupos, lo más puros posible.
        </p>

        <ul class="ejemplo-pronostico">
          <li>
            Por ejemplo, si al preguntar por el «Pronóstico» logras separar los días que jugaste tenis de los que no, y
            esto te permite identificar al clima como una variable que influye directamente en la decisión de jugar ese
            día,
            como que nunca hayas jugado en dia de lluvia.
          </li>
          <li>
            Esto convierte esta a pregunta en una una excelente primera pregunta, al permitir clasificar desde la
            primera interacción posibles casos.
          </li>
        </ul>

        <p>
          Esta elección no es al azar: se basa en criterios matemáticos como la <strong>«Ganancia de
            Información»</strong>
          o la <strong>«Impureza de Gini»</strong>, que miden cuánta claridad aporta cada división. Así, el árbol se
          construye de forma jerárquica, comenzando con las preguntas más relevantes hasta llegar a una conclusión
          final.
        </p>
      </div>


      <h3>Conceptos Clave (Definición Formal)</h3>
      <div class="concepto-clave">
        <h4>Nodo</h4>
        <p> Es un punto de decisión en el camino, donde el arbol necesitará decidir alguna característica, la cual
          obtendrá por medio de una pregunta, como por ejemplo: «¿La humedad es mayor al 70%?». Asimismo, dentro del
          arbol, el primer punto, donde comienza todo el recorrido del árbol, se llama nodo raíz.</p>

        <h4>Rama (o Arista)</h4>
        <p>Es el camino que se selecciona en base a la respuesta de la pregunta de un nodo. Para esto, se debe
          considerar que cada arista tiene un valor, el cual corresponde a la respuesta de la pregunta que representa,
          como un:
        </p>
        <ul>
          <li>
            Sí
          </li>
          <li>
            No
          </li>
          <li>
            Soleado
          </li>
          <li>
            Nublado
          </li>
        </ul>
        <h4>Hoja (o Nodo Terminal)</h4>
        <p>Es el nodo final al que se llega cuando no existen más preguntas en ese camino o rama, razón por la que se
          le dice <strong>«Hoja»</strong>. Finalmente, este nodo contiene la predicción final del modelo</p>
      </div>

      <div class="controls">
        <label for="depth-slider">Profundidad del Árbol:</label>
        <input type="range" id="depth-slider" min="1" max="4" value="4">
      </div>

      <div id="tree-visualization"></div>
      <h3>El Código Detrás de la Decisión</h3>
      <pre><code class="language-js">
          <span class="code-comment">// Esta función simula la lógica de nuestro árbol de decisión.</span>
          <span class="code-keyword">function</span> <span class="code-function">decidirSiJugarTenis</span>(<span
            class="code-param">pronostico</span>, <span class="code-param">humedad</span>, <span
            class="code-param">viento</span>) {

          <span class="code-comment"> // Primera pregunta (Nodo raíz)</span>
          <span class="code-keyword"> if</span> (pronostico === <span class="code-string">'Soleado'</span>) {

          <span class="code-comment">// Segunda pregunta</span>
          <span class="code-keyword">if</span> (humedad === <span class="code-string">'Alta'</span>) {
          <span class="code-keyword">return</span> <span class="code-string">'NO JUGAR'</span>; <span
            class="code-comment">// Hoja</span>
          } <span class="code-keyword">else if</span> (humedad === <span class="code-string">'Normal'</span>) {
          <span class="code-keyword">return</span> <span class="code-string">'SÍ JUGAR'</span>; <span
            class="code-comment">// Hoja</span>
          }

          } <span class="code-keyword">else if</span> (pronostico === <span class="code-string">'Nublado'</span>) {

          <span class="code-keyword">return</span> <span class="code-string">'SÍ JUGAR'</span>; <span
            class="code-comment">// Hoja</span>

          } <span class="code-keyword">else if</span> (pronostico === <span class="code-string">'Lluvioso'</span>) {

          <span class="code-comment">// Tercera pregunta</span>
          <span class="code-keyword">if</span> (viento === <span class="code-string">'Fuerte'</span>) {
          <span class="code-keyword">return</span> <span class="code-string">'NO JUGAR'</span>; <span
            class="code-comment">// Hoja</span>
          } <span class="code-keyword">else if</span> (viento === <span class="code-string">'Débil'</span>) {
          <span class="code-keyword">return</span> <span class="code-string">'SÍ JUGAR'</span>; <span
            class="code-comment">// Hoja</span>
          }
          }
          }

          <span class="code-comment">// Ejemplo de uso:</span>
          <span class="code-comment">// const decision = decidirSiJugarTenis('Soleado', 'Normal');</span>
          <span class="code-comment">// console.log(decision); // Resultado: 'SÍ JUGAR'</span>
        </code></pre>
    </section>

    <section id="reinforcement-learning">
      <h2>🤖 Aprendizaje por Refuerzo</h2>
      <p>Paradigma dentro del Machine Learning que basa su funcionamiento en la «Psicología Conductista». A
        diferencia del «Aprendizaje supervisado», donde se dan respuestas las respuestas correctas de inmediato, en el
        aprendizaje por refuerzo no se le dan las respuestas correctas. Esto se debe a que el sistema aprende a tomar
        decisiones
        a través de <strong>«Prueba y Error»</strong>, recibiendo o perdiendo una recomepensa por cada caso de éxito, la
        cual se va acumulando en el tiempo.
      <h3>La Idea en Simple: Explorar vs. Explotar</h3>
      <p>Imagina que estás en ciudad nueva, donde descubres que tienes un restaurante afuera de tu casa, ademas, tus
        amigos te recomendarón restaurantes buenos en la ciudad. En este punto, tienes dos opciones:
      <ul>
        <li><strong>Explotar</strong> los restaurantes recomendados por tus amigos, ya que sabes que estos son buenos
          (una recompensa segura).</li>
        <li><strong>«Explorar»</strong> y probar el restaurante nuevo y desconocido, el cual puede ser muy malo
          (castigo) o ser el mejor que hayas conocido (una recompensa mucho mayor).</li>
      </ul>

      Los agente RL se enfrentan con estos problemas, de decidir entre usar una estrategia que conoce (explotar) o
      probar
      algo nuevo con la esperanza de descubrir una estrategia aún mejor (explorar)
      </p>
      <h3>Conceptos Clave (Definición Formal)</h3>
      <div class="concepto-clave">
        <h4>Agente</h4>
        <p> Es la entidad «protagonista» que aprende y toma las decisiones, el cual funciona como un «aprendiz» del
          sistema. En el ejemplo del laberinto, el agente corresponde al círculo azul.</p>

        <h4>Entorno</h4>
        <p>Es el «universo» donde el agente existe y actúa. Para esto, el universo contiene
          sus propias reglas El entorno tiene sus propias, como el no se permitir atravesar muros en el ejemplo del
          laberinto, y responde a las acciones del agente dándole un nuevo estado y una
          recompensa.</p>
        <h4>Acción</h4>
        <p> Es una de las posibles acciones que el agente puede realizar desde su estado
          actual.</p>
        <h4>Recompensa</h4>
        <p>Es la retroalimentación numérica que el entorno le da al agente, la cual indica
          a este si su
          última acción fue buena (recompensa positiva), mala (negativa) o neutra, permitiendo aprender al agente.
        </p>
        <h4>Política</h4>
        <p> Es un «mapa» que le indica al agente cuál es la mejor acción a tomar en cada estado posible, con tal de
          maximizar
          la recompensa total a futuro.
          Para el ejemplo del laberinto, las flechas corresponden a una
          visualización de la política.</p>
        </p>
      </div>

      <div id="rl-simulation">
        <div id="rl-controls">
          <div class="slider-group">
            <div class="slider-container">
              <label for="alpha-slider">Tasa de Aprendizaje (&alpha;): <span id="alpha-value">0.1</span></label>
              <input type="range" id="alpha-slider" min="0.1" max="1" step="0.1" value="0.1">
            </div>
            <div class="slider-container">
              <label for="gamma-slider">Factor de Descuento (&gamma;): <span id="gamma-value">0.9</span></label>
              <input type="range" id="gamma-slider" min="0.1" max="1" step="0.1" value="0.9">
            </div>
            <div class="slider-container">
              <label for="epsilon-slider">Tasa de Exploración (&epsilon;): <span id="epsilon-value">0.2</span></label>
              <input type="range" id="epsilon-slider" min="0.1" max="1" step="0.1" value="0.2">
            </div>
          </div>
          <div class="slider-container speed-control">
            <label for="speed-slider">Velocidad de Simulación:</label>
            <input type="range" id="speed-slider" min="10" max="500" value="150">
          </div>
          <div class="info-group">
            <div class="info">Episodio: <span id="episode-count">0</span></div>
            <div class="info">Pasos: <span id="step-count">0</span></div>
            <div class="info">Última Recompensa: <span id="reward-display">0</span></div>
          </div>
          <div class="main-controls">
            <button id="start-pause-btn" class="btn">Iniciar Entrenamiento</button>
            <button id="reset-rl-btn" class="btn">Reiniciar Todo</button>
          </div>
        </div>
        <div id="rl-dashboard">
          <div id="maze-container"></div>
          <div id="rl-chart-container"></div>
        </div>
      </div>
    </section>

    <section id="conclusion">
      <h2>📊 Conclusiones y Comparativa</h2>

      <h4>El Paradigma de Aprendizaje: Datos vs. Interacción</h4>
      <p>La diferencia más importante entre estos modelos está en la forma en que aprenden, ya que un <strong>«Árbol de
          Decisión»</strong>
        funciona bajo un enfoque de Aprendizaje supervisado, como un estudiante que tiene un libro de matemáticas
        lleno de problemas y sus soluciones (un conjunto de datos etiquetado). Su tarea es estudiar ese material fijo y
        crear un conjunto de reglas que le permita resolver problemas similares en el futuro. Sin un nuevo conjunto de
        datos no se puede aprender nada nuevo.
      </p>
      <p>En cambio, el «Aprendizaje por Refuerzo» se sustenta de la interacción activa con su entorno, como un bebé que
        intenta aprender a caminar: no tiene un manual, sino que aprende a través de la experiencia. En este ejemplo, el
        bebe da un paso (una
        acción), recibe una señal (se cae o se mantiene en pie) y ajusta su comportamiento en función de esa
        retroalimentación. Su conocimiento se forma a través de prueba y error, todo con el objetivo de alcanzar una
        meta a largo plazo.</p>

      <h4>Interpretabilidad: La «Caja Blanca» vs. la «Caja Negra»</h4>
      <p>
        La «Caja Blanca» frente a la «Caja Negra»
        Una de las grandes ventajas de los Árboles de Decisión es su alta interpretabilidad; son un modelo de «caja
        blanca». Podemos analizar la estructura del árbol y seguir la secuencia de reglas que nos llevaron a una
        predicción específica. Esto es crucial en áreas como las finanzas o la medicina, donde es fundamental poder
        justificar una decisión.
      </p>
      <p>El R, a menudo, resulta en una <strong>«caja negra»</strong>. Aunque se puede observar la política final (las
        flechas en el laberinto), la lógica subyacente es una tabla compleja de valores (la Q-Table) que no ofrece
        una explicación intuitiva de por qué una acción es preferible a otra. El comportamiento óptimo «emerge» del
        entrenamiento, pero no se presenta como un conjunto de reglas legibles.</p>

      <div class="table-container">
        <table>
          <thead>
            <tr>
              <th>Característica</th>
              <th>Árboles de Decisión</th>
              <th>Aprendizaje por Refuerzo (RL)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Tipo de Aprendizaje</strong></td>
              <td>Supervisado (aprende de datos etiquetados)</td>
              <td>Por Interacción (aprende de recompensas y castigos)</td>
            </tr>
            <tr>
              <td><strong>Datos Necesarios</strong></td>
              <td>Un conjunto de datos (dataset) con ejemplos y sus resultados correctos.</td>
              <td>Un entorno definido, un conjunto de acciones y una función de recompensa.</td>
            </tr>
            <tr>
              <td><strong>Objetivo Principal</strong></td>
              <td>Predecir un valor o clasificar una entidad basándose en sus características.</td>
              <td>Aprender una secuencia de acciones para maximizar una recompensa a largo plazo.</td>
            </tr>
            <tr>
              <td><strong>Interpretabilidad</strong></td>
              <td>Muy alta. Las reglas de decisión son fáciles de leer y entender (caja blanca).</td>
              <td>Variable. La política puede ser simple o extremadamente compleja y difícil de interpretar.</td>
            </tr>
            <tr>
              <td><strong>Aplicaciones Típicas</strong></td>
              <td>Diagnóstico médico, aprobación de créditos, segmentación de clientes.</td>
              <td>Robótica, control de sistemas, entrenamiento de IAs para juegos, optimización de rutas.</td>
            </tr>
          </tbody>
        </table>
      </div>

      <h3>Recomendaciones Finales</h3>
      <p>
        Este proyecto muestra que la mejor manera de entender un algoritmo es viéndolo en acción. Es recomendable
        ajustar la profundidad del árbol para observar el sobreajuste, o cambiar los parámetros del agente de RL para
        notar cómo se vuelve más «impaciente» o «curioso». La intuición que adquieres a través de estas interacciones es
        realmente invaluable.
      </p>
    </section>
  </main>

  <footer>
    <p>&copy; 2025 - Universidad de La Frontera</p>
    <p><a href="https://github.com/axiao1134/proyectoFinal_sistemas_inteligentes" target="_blank">Ver Código en
        GitHub</a></p>
  </footer>

  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script type="module" src="js/index.js"></script>
</body>

</html>
