<!DOCTYPE html>
<html lang="es">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ML-Explainer: Una Aventura Interactiva</title>
  <link rel="stylesheet" href="css/styles.css">
</head>

<body>

  <header>
    <h1>ML-Explainer: Una Aventura Interactiva</h1>
    <nav>
      <a href="#decision-trees">츼rboles de Decisi칩n</a>
      <a href="#reinforcement-learning">Reinforcement Learning</a>
      <a href="#conclusion">Conclusi칩n</a>
    </nav>
  </header>

  <main>
    <section id="portada">
      <h2>Comprende el Machine Learning de Forma Interactiva</h2>
      <p><strong>Bienvenido a ML-Explainer.</strong> Esta p치gina es un laboratorio interactivo dise침ado para
        desmitificar dos de los conceptos m치s poderosos de la inteligencia artificial. Aqu칤, no solo leer치s sobre los
        algoritmos, sino que podr치s jugar con ellos, ajustar sus par치metros y ver c칩mo "piensan" en tiempo real.</p>
      <p><strong>Autor:</strong> Zihao Xiao/Jes칰s Tapia/Benjam칤n Fern치ndez</p>
    </section>

    <section id="decision-trees">
      <h2>游꺕 츼rboles de Decisi칩n</h2>
      <p>Los 치rboles de decisi칩n son uno de los algoritmos de <strong>aprendizaje supervisado</strong> m치s intuitivos
        del Machine Learning. Se utilizan principalmente para tareas de <strong>clasificaci칩n</strong> (predecir una
        categor칤a) y <strong>regresi칩n</strong> (predecir un valor num칠rico), imitando la forma en que los humanos toman
        decisiones mediante una serie de preguntas.</p>
      <h3>La Idea en Simple: Un Juego de Preguntas Inteligente</h3>
      <p>Imagina que quieres decidir si jugar tenis. Un 치rbol de decisi칩n te gu칤a con preguntas, pero no son preguntas
        al azar. El algoritmo busca la "pregunta m치s inteligente" en cada paso, aquella que mejor divide los datos en
        grupos lo m치s puros posible. Por ejemplo, si al preguntar por el `Pron칩stico` separamos perfectamente los d칤as
        de juego de los de no juego, esa ser칤a una excelente primera pregunta. Este proceso de selecci칩n se basa en
        criterios matem치ticos como la <strong>Ganancia de Informaci칩n</strong> o la <strong>Impureza de Gini</strong>,
        que miden cu치nta "claridad" ganamos con cada pregunta. El 치rbol se construye de forma jer치rquica, haciendo las
        preguntas m치s importantes primero, hasta llegar a una conclusi칩n.</p>

      <h3>Conceptos Clave (Definici칩n Formal)</h3>
      <ul>
        <li><strong>Nodo:</strong> Es como una encrucijada en el camino. En este punto, se eval칰a una caracter칤stica de
          los datos a trav칠s de una pregunta (`&iquest;Humedad > 70%?`). El primer nodo, desde donde parte todo, se
          llama <strong>nodo ra칤z</strong>.</li>
        <li><strong>Rama (o Arista):</strong> Es el camino que se toma despu칠s de responder la pregunta en un nodo. Cada
          rama est치 etiquetada con una posible respuesta (`S칤`, `No`, `Soleado`, `Nublado`) y te conduce a la siguiente
          pregunta o a la conclusi칩n final.</li>
        <li><strong>Hoja (o Nodo Terminal):</strong> Es el destino final de un camino. Aqu칤 no hay m치s preguntas. La
          <strong>hoja</strong> contiene la predicci칩n final del modelo (`S칈 JUGAR`). Todas las muestras de datos que
          recorren el mismo camino hasta una hoja reciben la misma predicci칩n.
        </li>
      </ul>

      <div class="controls">
        <label for="depth-slider">Profundidad del 츼rbol:</label>
        <input type="range" id="depth-slider" min="1" max="4" value="4">
      </div>
      <div id="tree-visualization"></div>
      <h3>El C칩digo Detr치s de la Decisi칩n</h3>
      <pre><code class="language-js">
          <span class="code-comment">// Esta funci칩n simula la l칩gica de nuestro 치rbol de decisi칩n.</span>
          <span class="code-keyword">function</span> <span class="code-function">decidirSiJugarTenis</span>(<span
            class="code-param">pronostico</span>, <span class="code-param">humedad</span>, <span
            class="code-param">viento</span>) {

          <span class="code-comment">// Primera pregunta (Nodo ra칤z)</span>
          <span class="code-keyword">if</span> (pronostico === <span class="code-string">'Soleado'</span>) {

          <span class="code-comment">// Segunda pregunta</span>
          <span class="code-keyword">if</span> (humedad === <span class="code-string">'Alta'</span>) {
          <span class="code-keyword">return</span> <span class="code-string">'NO JUGAR'</span>; <span
            class="code-comment">// Hoja</span>
          } <span class="code-keyword">else if</span> (humedad === <span class="code-string">'Normal'</span>) {
          <span class="code-keyword">return</span> <span class="code-string">'S칈 JUGAR'</span>; <span
            class="code-comment">// Hoja</span>
          }

          } <span class="code-keyword">else if</span> (pronostico === <span class="code-string">'Nublado'</span>) {

          <span class="code-keyword">return</span> <span class="code-string">'S칈 JUGAR'</span>; <span
            class="code-comment">// Hoja</span>

          } <span class="code-keyword">else if</span> (pronostico === <span class="code-string">'Lluvioso'</span>) {

          <span class="code-comment">// Tercera pregunta</span>
          <span class="code-keyword">if</span> (viento === <span class="code-string">'Fuerte'</span>) {
          <span class="code-keyword">return</span> <span class="code-string">'NO JUGAR'</span>; <span
            class="code-comment">// Hoja</span>
          } <span class="code-keyword">else if</span> (viento === <span class="code-string">'D칠bil'</span>) {
          <span class="code-keyword">return</span> <span class="code-string">'S칈 JUGAR'</span>; <span
            class="code-comment">// Hoja</span>
          }
          }
          }

          <span class="code-comment">// Ejemplo de uso:</span>
          <span class="code-comment">// const decision = decidirSiJugarTenis('Soleado', 'Normal');</span>
          <span class="code-comment">// console.log(decision); // Resultado: 'S칈 JUGAR'</span>
        </code></pre>
    </section>

    <section id="reinforcement-learning">
      <h2>游뱄 Aprendizaje por Refuerzo</h2>
      <p>El Aprendizaje por Refuerzo (RL) es un paradigma de Machine Learning inspirado en la psicolog칤a conductista. A
        diferencia del aprendizaje supervisado, no se le dan respuestas correctas. En su lugar, el sistema aprende a
        tomar decisiones secuenciales a trav칠s de la <strong>prueba y el error</strong>, interactuando con un entorno
        para maximizar una recompensa acumulada a largo plazo.</p>
      <h3>La Idea en Simple: Explorar vs. Explotar</h3>
      <p>Piensa en la analog칤a de "entrenar a una mascota", pero vamos un paso m치s all치. Imagina que est치s en una ciudad
        nueva y encuentras un restaurante bueno cerca de tu hotel. Tienes dos opciones cada noche:
        <strong>Explotar</strong> tu conocimiento y volver al restaurante que ya sabes que es bueno (una recompensa
        segura), o **Explorar** y probar un restaurante nuevo y desconocido. Este nuevo lugar podr칤a ser terrible
        (castigo) o podr칤a ser el mejor de la ciudad (una recompensa mucho mayor). El agente de RL enfrenta
        constantemente este dilema: 쯗eber칤a usar la mejor estrategia que conoce hasta ahora (explotar) o deber칤a probar
        algo nuevo con la esperanza de descubrir una estrategia a칰n mejor (explorar)? El equilibrio entre estas dos
        acciones es fundamental para un buen aprendizaje.
      </p>
      <h3>Conceptos Clave (Definici칩n Formal)</h3>
      <ul>
        <li><strong>Agente:</strong> Es el 'protagonista' que aprende y toma las decisiones. No es solo un objeto, sino
          el 'aprendiz' del sistema. En nuestro laberinto, es el c칤rculo azul.</li>
        <li><strong>Entorno:</strong> Es el 'universo' donde el agente existe y act칰a. El entorno tiene sus propias
          reglas (ej. "no se puede atravesar muros") y responde a las acciones del agente d치ndole un nuevo estado y una
          recompensa.</li>
        <li><strong>Acci칩n:</strong> Es una de las jugadas posibles que el agente puede realizar desde su estado actual.
          El conjunto de todas las acciones posibles (arriba, abajo, etc.) define su capacidad de movimiento.</li>
        <li><strong>Recompensa:</strong> Es la retroalimentaci칩n num칠rica que el entorno le da al agente. Le dice si su
          칰ltima acci칩n fue buena (recompensa positiva), mala (negativa) o neutra. Es el principal motor del
          aprendizaje.</li>
        <li><strong>Pol칤tica (Policy):</strong> Esta es la pieza m치s importante. Es el 'cerebro' o la estrategia que el
          agente desarrolla. Una pol칤tica es un mapa que le dice al agente cu치l es la mejor acci칩n a tomar en cada
          estado posible para maximizar la recompensa total a futuro. Las flechas en nuestro laberinto son una
          visualizaci칩n de la pol칤tica.</li>
      </ul>

      <div id="rl-simulation">
        <div id="rl-controls">
          <div class="slider-group">
            <div class="slider-container">
              <label for="alpha-slider">Tasa de Aprendizaje (&alpha;): <span id="alpha-value">0.1</span></label>
              <input type="range" id="alpha-slider" min="0.1" max="1" step="0.1" value="0.1">
            </div>
            <div class="slider-container">
              <label for="gamma-slider">Factor de Descuento (&gamma;): <span id="gamma-value">0.9</span></label>
              <input type="range" id="gamma-slider" min="0.1" max="1" step="0.1" value="0.9">
            </div>
            <div class="slider-container">
              <label for="epsilon-slider">Tasa de Exploraci칩n (&epsilon;): <span id="epsilon-value">0.2</span></label>
              <input type="range" id="epsilon-slider" min="0.1" max="1" step="0.1" value="0.2">
            </div>
          </div>
          <div class="control-group main-controls">
            <button id="start-pause-btn" class="btn">Iniciar Entrenamiento</button>
            <button id="reset-rl-btn" class="btn">Reiniciar Todo</button>
          </div>
          <div class="slider-container speed-control">
            <label for="speed-slider">Velocidad de Simulaci칩n:</label>
            <input type="range" id="speed-slider" min="10" max="500" value="150">
          </div>
          <div class="info-group">
            <div class="info">Episodio: <span id="episode-count">0</span></div>
            <div class="info">Pasos: <span id="step-count">0</span></div>
            <div class="info">칔ltima Recompensa: <span id="reward-display">0</span></div>
          </div>
        </div>
        <div id="rl-dashboard">
          <div id="maze-container"></div>
          <div id="rl-chart-container"></div>
        </div>
      </div>
    </section>

    <section id="conclusion">
      <h2>游늵 Conclusiones y Comparativa</h2>

      <h4>El Paradigma de Aprendizaje: Datos vs. Interacci칩n</h4>
      <p>La diferencia m치s fundamental radica en <em>c칩mo</em> aprende cada modelo. Un <strong>츼rbol de
          Decisi칩n</strong> opera bajo un paradigma de <strong>aprendizaje supervisado</strong>. Es como un estudiante
        al que se le entrega un libro de texto con todos los problemas y sus soluciones (un dataset etiquetado). Su
        objetivo es estudiar este material est치tico y crear un conjunto de reglas claras para poder resolver problemas
        similares en el futuro. No puede aprender nada nuevo sin un nuevo conjunto de datos.</p>
      <p>Por el contrario, el <strong>Aprendizaje por Refuerzo</strong> aprende de la <strong>interacci칩n
          din치mica</strong> con un entorno. Es como un beb칠 aprendiendo a caminar: no tiene un manual, sino que aprende
        a trav칠s de la experiencia. Realiza una acci칩n (un paso), recibe una se침al (se cae o se mantiene en pie) y
        ajusta su comportamiento futuro en base a esa retroalimentaci칩n. Su conocimiento se construye a trav칠s de la
        prueba y el error para lograr un objetivo a largo plazo.</p>

      <h4>Interpretabilidad: La "Caja Blanca" vs. la "Caja Negra"</h4>
      <p>La gran ventaja de los 츼rboles de Decisi칩n es su alta interpretabilidad; son un modelo de <strong>"caja
          blanca"</strong>. Podemos examinar la estructura del 치rbol y seguir exactamente la secuencia de reglas que
        llevaron a una predicci칩n espec칤fica. Esto es vital en campos como las finanzas o la medicina, donde es crucial
        poder justificar una decisi칩n.</p>
      <p>El RL, a menudo, resulta en una <strong>"caja negra"</strong>. Aunque podemos observar la pol칤tica final (las
        flechas en nuestro laberinto), la l칩gica subyacente es una compleja tabla de valores (la Q-Table) que no ofrece
        una explicaci칩n intuitiva de por qu칠 una acci칩n es preferible a otra. El comportamiento 칩ptimo "emerge" del
        entrenamiento, pero no se presenta como un conjunto de reglas legibles.</p>

      <div class="table-container">
        <table>
          <thead>
            <tr>
              <th>Caracter칤stica</th>
              <th>츼rboles de Decisi칩n</th>
              <th>Aprendizaje por Refuerzo (RL)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Tipo de Aprendizaje</strong></td>
              <td>Supervisado (aprende de datos etiquetados)</td>
              <td>Por Interacci칩n (aprende de recompensas y castigos)</td>
            </tr>
            <tr>
              <td><strong>Datos Necesarios</strong></td>
              <td>Un conjunto de datos (dataset) con ejemplos y sus resultados correctos.</td>
              <td>Un entorno definido, un conjunto de acciones y una funci칩n de recompensa.</td>
            </tr>
            <tr>
              <td><strong>Objetivo Principal</strong></td>
              <td>Predecir un valor o clasificar una entidad bas치ndose en sus caracter칤sticas.</td>
              <td>Aprender una secuencia de acciones para maximizar una recompensa a largo plazo.</td>
            </tr>
            <tr>
              <td><strong>Interpretabilidad</strong></td>
              <td>Muy alta. Las reglas de decisi칩n son f치ciles de leer y entender (caja blanca).</td>
              <td>Variable. La pol칤tica puede ser simple o extremadamente compleja y dif칤cil de interpretar.</td>
            </tr>
            <tr>
              <td><strong>Aplicaciones T칤picas</strong></td>
              <td>Diagn칩stico m칠dico, aprobaci칩n de cr칠ditos, segmentaci칩n de clientes.</td>
              <td>Rob칩tica, control de sistemas, entrenamiento de IAs para juegos, optimizaci칩n de rutas.</td>
            </tr>
          </tbody>
        </table>
      </div>

      <h3>Recomendaciones Finales</h3>
      <p>Este proyecto demuestra que la mejor forma de entender un algoritmo es vi칠ndolo en acci칩n. Te animamos a
        experimentar: ajusta la profundidad del 치rbol para ver el sobreajuste, o modifica los par치metros del agente de
        RL para ver c칩mo se vuelve m치s "impaciente" o "curioso". La intuici칩n que se gana con estas interacciones es
        invaluable.</p>
    </section>
  </main>

  <footer>
    <p>&copy; 2025 - Universidad de La Frontera</p>
    <p><a href="https://github.com/axiao1134/proyectoFinal_sistemas_inteligentes" target="_blank">Ver C칩digo en
        GitHub</a></p>
  </footer>

  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script type="module" src="js/index.js"></script>
</body>

</html>
