<!DOCTYPE html>
<html lang="es">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ML-Explainer: Una Aventura Interactiva</title>
  <link rel="stylesheet" href="css/styles.css">
</head>

<body>

  <header>
    <h1>ML-Explainer: Una Aventura Interactiva</h1>
    <nav>
      <a href="#decision-trees">츼rboles de Decisi칩n</a>
      <a href="#reinforcement-learning">Reinforcement Learning</a>
      <a href="#conclusion">Conclusi칩n</a>
    </nav>
  </header>

  <main>
    <section id="portada">
      <h3 class="bienvenida">춰Bienvenido a ML-Explainer!</h3>
      <h2 class="subtitulo-portada">Comprende el Machine Learning de Forma Interactiva</h2>
      <p>Esta p치gina es un laboratorio interactivo dise침ado para desmitificar dos de los conceptos m치s poderosos del
        Machine Learning. Aqu칤, no solo leer치s sobre los algoritmos detr치s de este, sino que podr치s jugar con
        ellos, ajustar sus par치metros y ver c칩mo 춺piensan췉 en tiempo real.</p>
      <div class="autores">
        <span>Zihao Xiao</span>
        <span>Jes칰s Tapia</span>
        <span>Benjam칤n Fern치ndez</span>
      </div>
    </section>

    <section id="decision-trees">
      <h2>游꺕 츼rboles de Decisi칩n</h2>
      <p>Dentro del Machine Learning, los 치rboles de decisi칩n son de los algoritmos de <strong>춺Aprendizaje
          Supervisado췉</strong> m치s intuitivos
        que hay. Estos son utilizados principalmente para tareas de <strong>춺Clasificaci칩n췉</strong>, como predecir una
        categor칤a, y tareas de <strong>춺Regresi칩n췉</strong>, como predecir un valor num칠rico, imitando la forma en que
        los humanos toman
        decisiones mediante una serie de preguntas.</p>
      <h3>La Idea en Simple: Un Juego de Preguntas Inteligente</h3>
      <div class="explicacion-arbol">
        <p>
          Imagina que quieres decidir si jugar o no jugar tenis, para esto un 치rbol de decisi칩n te guiar치 por medio de
          preguntas de manera aleatoria.
          una serie de preguntas.
          Bajo este contexto, se debe considerar que el algoritmo buscar치 la 춺pregunta m치s inteligente췉 en cada paso, es
          decir, aquella que mejor
          divida los datos en grupos, lo m치s puros posible.
        </p>

        <ul class="ejemplo-pronostico">
          <li>
            Por ejemplo, si al preguntar por el 춺Pron칩stico췉 logras separar los d칤as que jugaste tenis de los que no, y
            esto te permite identificar al clima como una variable que influye directamente en la decisi칩n de jugar ese
            d칤a,
            como que nunca hayas jugado en dia de lluvia.
          </li>
          <li>
            Esto convierte esta a pregunta en una una excelente primera pregunta, al permitir clasificar desde la
            primera interacci칩n posibles casos.
          </li>
        </ul>

        <p>
          Esta elecci칩n no es al azar: se basa en criterios matem치ticos como la <strong>춺Ganancia de
            Informaci칩n췉</strong>
          o la <strong>춺Impureza de Gini췉</strong>, que miden cu치nta claridad aporta cada divisi칩n. As칤, el 치rbol se
          construye de forma jer치rquica, comenzando con las preguntas m치s relevantes hasta llegar a una conclusi칩n
          final.
        </p>
      </div>


      <h3>Conceptos Clave (Definici칩n Formal)</h3>
      <div class="concepto-clave">
        <h4>Nodo</h4>
        <p> Es un punto de decisi칩n en el camino, donde el arbol necesitar치 decidir alguna caracter칤stica, la cual
          obtendr치 por medio de una pregunta, como por ejemplo: 춺쯃a humedad es mayor al 70%?췉. Asimismo, dentro del
          arbol, el primer punto, donde comienza todo el recorrido del 치rbol, se llama nodo ra칤z.</p>

        <h4>Rama (o Arista)</h4>
        <p>Es el camino que se selecciona en base a la respuesta de la pregunta de un nodo. Para esto, se debe
          considerar que cada arista tiene un valor, el cual corresponde a la respuesta de la pregunta que representa,
          como un:
        </p>
        <ul>
          <li>
            S칤
          </li>
          <li>
            No
          </li>
          <li>
            Soleado
          </li>
          <li>
            Nublado
          </li>
        </ul>
        <h4>Hoja (o Nodo Terminal)</h4>
        <p>Es el nodo final al que se llega cuando no existen m치s preguntas en ese camino o rama, raz칩n por la que se
          le dice <strong>춺Hoja췉</strong>. Finalmente, este nodo contiene la predicci칩n final del modelo</p>
      </div>

      <div class="controls">
        <label for="depth-slider">Profundidad del 츼rbol:</label>
        <input type="range" id="depth-slider" min="1" max="4" value="4">
      </div>

      <div id="tree-visualization"></div>
      <h3>El C칩digo Detr치s de la Decisi칩n</h3>
      <pre><code class="language-js">
          <span class="code-comment">// Esta funci칩n simula la l칩gica de nuestro 치rbol de decisi칩n.</span>
          <span class="code-keyword">function</span> <span class="code-function">decidirSiJugarTenis</span>(<span
            class="code-param">pronostico</span>, <span class="code-param">humedad</span>, <span
            class="code-param">viento</span>) {

          <span class="code-comment"> // Primera pregunta (Nodo ra칤z)</span>
          <span class="code-keyword"> if</span> (pronostico === <span class="code-string">'Soleado'</span>) {

          <span class="code-comment">// Segunda pregunta</span>
          <span class="code-keyword">if</span> (humedad === <span class="code-string">'Alta'</span>) {
          <span class="code-keyword">return</span> <span class="code-string">'NO JUGAR'</span>; <span
            class="code-comment">// Hoja</span>
          } <span class="code-keyword">else if</span> (humedad === <span class="code-string">'Normal'</span>) {
          <span class="code-keyword">return</span> <span class="code-string">'S칈 JUGAR'</span>; <span
            class="code-comment">// Hoja</span>
          }

          } <span class="code-keyword">else if</span> (pronostico === <span class="code-string">'Nublado'</span>) {

          <span class="code-keyword">return</span> <span class="code-string">'S칈 JUGAR'</span>; <span
            class="code-comment">// Hoja</span>

          } <span class="code-keyword">else if</span> (pronostico === <span class="code-string">'Lluvioso'</span>) {

          <span class="code-comment">// Tercera pregunta</span>
          <span class="code-keyword">if</span> (viento === <span class="code-string">'Fuerte'</span>) {
          <span class="code-keyword">return</span> <span class="code-string">'NO JUGAR'</span>; <span
            class="code-comment">// Hoja</span>
          } <span class="code-keyword">else if</span> (viento === <span class="code-string">'D칠bil'</span>) {
          <span class="code-keyword">return</span> <span class="code-string">'S칈 JUGAR'</span>; <span
            class="code-comment">// Hoja</span>
          }
          }
          }

          <span class="code-comment">// Ejemplo de uso:</span>
          <span class="code-comment">// const decision = decidirSiJugarTenis('Soleado', 'Normal');</span>
          <span class="code-comment">// console.log(decision); // Resultado: 'S칈 JUGAR'</span>
        </code></pre>
    </section>

    <section id="reinforcement-learning">
      <h2>游뱄 Aprendizaje por Refuerzo</h2>
      <p>Paradigma dentro del Machine Learning que basa su funcionamiento en la 춺Psicolog칤a Conductista췉. A
        diferencia del 춺Aprendizaje supervisado췉, donde se dan respuestas las respuestas correctas de inmediato, en el
        aprendizaje por refuerzo no se le dan las respuestas correctas. Esto se debe a que el sistema aprende a tomar
        decisiones
        a trav칠s de <strong>춺Prueba y Error췉</strong>, recibiendo o perdiendo una recomepensa por cada caso de 칠xito, la
        cual se va acumulando en el tiempo.
      <h3>La Idea en Simple: Explorar vs. Explotar</h3>
      <p>Imagina que est치s en ciudad nueva, donde descubres que tienes un restaurante afuera de tu casa, ademas, tus
        amigos te recomendar칩n restaurantes buenos en la ciudad. En este punto, tienes dos opciones:
      <ul>
        <li><strong>Explotar</strong> los restaurantes recomendados por tus amigos, ya que sabes que estos son buenos
          (una recompensa segura).</li>
        <li><strong>춺Explorar췉</strong> y probar el restaurante nuevo y desconocido, el cual puede ser muy malo
          (castigo) o ser el mejor que hayas conocido (una recompensa mucho mayor).</li>
      </ul>

      Los agente RL se enfrentan con estos problemas, de decidir entre usar una estrategia que conoce (explotar) o
      probar
      algo nuevo con la esperanza de descubrir una estrategia a칰n mejor (explorar)
      </p>
      <h3>Conceptos Clave (Definici칩n Formal)</h3>
      <div class="concepto-clave">
        <h4>Agente</h4>
        <p> Es la entidad 춺protagonista췉 que aprende y toma las decisiones, el cual funciona como un 춺aprendiz췉 del
          sistema. En el ejemplo del laberinto, el agente corresponde al c칤rculo azul.</p>

        <h4>Entorno</h4>
        <p>Es el 춺universo췉 donde el agente existe y act칰a. Para esto, el universo contiene
          sus propias reglas El entorno tiene sus propias, como el no se permitir atravesar muros en el ejemplo del
          laberinto, y responde a las acciones del agente d치ndole un nuevo estado y una
          recompensa.</p>
        <h4>Acci칩n</h4>
        <p> Es una de las posibles acciones que el agente puede realizar desde su estado
          actual.</p>
        <h4>Recompensa</h4>
        <p>Es la retroalimentaci칩n num칠rica que el entorno le da al agente, la cual indica
          a este si su
          칰ltima acci칩n fue buena (recompensa positiva), mala (negativa) o neutra, permitiendo aprender al agente.
        </p>
        <h4>Pol칤tica</h4>
        <p> Es un 춺mapa췉 que le indica al agente cu치l es la mejor acci칩n a tomar en cada estado posible, con tal de
          maximizar
          la recompensa total a futuro.
          Para el ejemplo del laberinto, las flechas corresponden a una
          visualizaci칩n de la pol칤tica.</p>
        </p>
      </div>

      <div id="rl-simulation">
        <div id="rl-controls">
          <div class="slider-group">
            <div class="slider-container">
              <label for="alpha-slider">Tasa de Aprendizaje (&alpha;): <span id="alpha-value">0.1</span></label>
              <input type="range" id="alpha-slider" min="0.1" max="1" step="0.1" value="0.1">
            </div>
            <div class="slider-container">
              <label for="gamma-slider">Factor de Descuento (&gamma;): <span id="gamma-value">0.9</span></label>
              <input type="range" id="gamma-slider" min="0.1" max="1" step="0.1" value="0.9">
            </div>
            <div class="slider-container">
              <label for="epsilon-slider">Tasa de Exploraci칩n (&epsilon;): <span id="epsilon-value">0.2</span></label>
              <input type="range" id="epsilon-slider" min="0.1" max="1" step="0.1" value="0.2">
            </div>
          </div>
          <div class="slider-container speed-control">
            <label for="speed-slider">Velocidad de Simulaci칩n:</label>
            <input type="range" id="speed-slider" min="10" max="500" value="150">
          </div>
          <div class="info-group">
            <div class="info">Episodio: <span id="episode-count">0</span></div>
            <div class="info">Pasos: <span id="step-count">0</span></div>
            <div class="info">칔ltima Recompensa: <span id="reward-display">0</span></div>
          </div>
          <div class="main-controls">
            <button id="start-pause-btn" class="btn">Iniciar Entrenamiento</button>
            <button id="reset-rl-btn" class="btn">Reiniciar Todo</button>
          </div>
        </div>
        <div id="rl-dashboard">
          <div id="maze-container"></div>
          <div id="rl-chart-container"></div>
        </div>
      </div>
    </section>

    <section id="conclusion">
      <h2>游늵 Conclusiones y Comparativa</h2>

      <h4>El Paradigma de Aprendizaje: Datos vs. Interacci칩n</h4>
      <p>La diferencia m치s importante entre estos modelos est치 en la forma en que aprenden, ya que un <strong>춺츼rbol de
          Decisi칩n췉</strong>
        funciona bajo un enfoque de Aprendizaje supervisado, como un estudiante que tiene un libro de matem치ticas
        lleno de problemas y sus soluciones (un conjunto de datos etiquetado). Su tarea es estudiar ese material fijo y
        crear un conjunto de reglas que le permita resolver problemas similares en el futuro. Sin un nuevo conjunto de
        datos no se puede aprender nada nuevo.
      </p>
      <p>En cambio, el 춺Aprendizaje por Refuerzo췉 se sustenta de la interacci칩n activa con su entorno, como un beb칠 que
        intenta aprender a caminar: no tiene un manual, sino que aprende a trav칠s de la experiencia. En este ejemplo, el
        bebe da un paso (una
        acci칩n), recibe una se침al (se cae o se mantiene en pie) y ajusta su comportamiento en funci칩n de esa
        retroalimentaci칩n. Su conocimiento se forma a trav칠s de prueba y error, todo con el objetivo de alcanzar una
        meta a largo plazo.</p>

      <h4>Interpretabilidad: La 춺Caja Blanca췉 vs. la 춺Caja Negra췉</h4>
      <p>
        La 춺Caja Blanca췉 frente a la 춺Caja Negra췉
        Una de las grandes ventajas de los 츼rboles de Decisi칩n es su alta interpretabilidad; son un modelo de 춺caja
        blanca췉. Podemos analizar la estructura del 치rbol y seguir la secuencia de reglas que nos llevaron a una
        predicci칩n espec칤fica. Esto es crucial en 치reas como las finanzas o la medicina, donde es fundamental poder
        justificar una decisi칩n.
      </p>
      <p>El R, a menudo, resulta en una <strong>춺caja negra췉</strong>. Aunque se puede observar la pol칤tica final (las
        flechas en el laberinto), la l칩gica subyacente es una tabla compleja de valores (la Q-Table) que no ofrece
        una explicaci칩n intuitiva de por qu칠 una acci칩n es preferible a otra. El comportamiento 칩ptimo 춺emerge췉 del
        entrenamiento, pero no se presenta como un conjunto de reglas legibles.</p>

      <div class="table-container">
        <table>
          <thead>
            <tr>
              <th>Caracter칤stica</th>
              <th>츼rboles de Decisi칩n</th>
              <th>Aprendizaje por Refuerzo (RL)</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Tipo de Aprendizaje</strong></td>
              <td>Supervisado (aprende de datos etiquetados)</td>
              <td>Por Interacci칩n (aprende de recompensas y castigos)</td>
            </tr>
            <tr>
              <td><strong>Datos Necesarios</strong></td>
              <td>Un conjunto de datos (dataset) con ejemplos y sus resultados correctos.</td>
              <td>Un entorno definido, un conjunto de acciones y una funci칩n de recompensa.</td>
            </tr>
            <tr>
              <td><strong>Objetivo Principal</strong></td>
              <td>Predecir un valor o clasificar una entidad bas치ndose en sus caracter칤sticas.</td>
              <td>Aprender una secuencia de acciones para maximizar una recompensa a largo plazo.</td>
            </tr>
            <tr>
              <td><strong>Interpretabilidad</strong></td>
              <td>Muy alta. Las reglas de decisi칩n son f치ciles de leer y entender (caja blanca).</td>
              <td>Variable. La pol칤tica puede ser simple o extremadamente compleja y dif칤cil de interpretar.</td>
            </tr>
            <tr>
              <td><strong>Aplicaciones T칤picas</strong></td>
              <td>Diagn칩stico m칠dico, aprobaci칩n de cr칠ditos, segmentaci칩n de clientes.</td>
              <td>Rob칩tica, control de sistemas, entrenamiento de IAs para juegos, optimizaci칩n de rutas.</td>
            </tr>
          </tbody>
        </table>
      </div>

      <h3>Recomendaciones Finales</h3>
      <p>
        Este proyecto muestra que la mejor manera de entender un algoritmo es vi칠ndolo en acci칩n. Es recomendable
        ajustar la profundidad del 치rbol para observar el sobreajuste, o cambiar los par치metros del agente de RL para
        notar c칩mo se vuelve m치s 춺impaciente췉 o 춺curioso췉. La intuici칩n que adquieres a trav칠s de estas interacciones es
        realmente invaluable.
      </p>
    </section>
  </main>

  <footer>
    <p>&copy; 2025 - Universidad de La Frontera</p>
    <p><a href="https://github.com/axiao1134/proyectoFinal_sistemas_inteligentes" target="_blank">Ver C칩digo en
        GitHub</a></p>
  </footer>

  <script src="https://d3js.org/d3.v7.min.js"></script>
  <script type="module" src="js/index.js"></script>
</body>

</html>
